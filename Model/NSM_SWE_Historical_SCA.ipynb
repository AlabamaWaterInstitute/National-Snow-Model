{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run on virtual environment: Snow_env with the following packages installed\n",
    "***tested on Python 3.9.12, does not work with python 3.10***\n",
    "\n",
    "Load all dependencies via cmd and pay special attention to version requirements.\n",
    "\n",
    "Steps for installing virtual env: https://janakiev.com/blog/jupyter-virtual-envs/\n",
    "\n",
    "1st geopandas: https://towardsdatascience.com/geopandas-installation-the-easy-way-for-windows-31a666b3610f\n",
    "Here are binaries: https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "2nd: rioxarray\n",
    "3rd: rasterio\n",
    "tables\n",
    "sklearn\n",
    "earthpy\n",
    "numpy 1.22\n",
    "pyshp 2.1\n",
    "h5py\n",
    "tqdm\n",
    "matplotlib 3.5\n",
    "seaborn\n",
    "tensorflow\n",
    "\n",
    "\n",
    "\n",
    "joblib\n",
    "pillow\n",
    "panda\n",
    "pyproj\n",
    "shapely\n",
    "kiwisolver\n",
    "fonttools\n",
    "pillow\n",
    "mpl_toolkits - pip install --user basemap\n",
    "**git clone https://github.com/matplotlib/matplotlib\n",
    "cd matplotlib\n",
    "pip install -e .**\n",
    "tables\n",
    "pytables\n",
    "netCDF4 **https://unidata.github.io/netcdf4-python/\n",
    "\n",
    "Data Assimilation of snotel data\n",
    "https://snowex-hackweek.github.io/website/tutorials/geospatial/SNOTEL_query.html\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import National_Snow_Model\n",
    "import NSM_SCA\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set path directory\n",
    "os.getcwd()\n",
    "os.chdir('..')  # TODO find a way around this, it causes issues when rerunning the code\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a1216932",
   "metadata": {},
   "source": [
    "## Running the NSM to create retrospective datasets\n",
    "\n",
    "The following datasets need to be 'created' within the Predictions folder, replacing YEAR with beginning of the desired water year:\n",
    "* predictionsYEAR-09-24.h5\n",
    "* submission_format_YEAR-09-24.csv\n",
    "\n",
    "The following datasets need to be 'created' within the Pre_Processed_DA folder, replacing YEAR with beginning of the desired water year:\n",
    "* ground_measures_features_YEAR-09-24.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rename columns in h5 file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c30500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "#the year of the original file\n",
    "prev_year = '2020'\n",
    "prev_date = prev_year + '-09-24'\n",
    "\n",
    "#input the new year of choice\n",
    "water_year = '2021'\n",
    "new_date = water_year + '-09-24'\n",
    "\n",
    "# TODO do the CSV files need to be updated as well?\n",
    "\n",
    "#for h5 files\n",
    "Region_list = ['N_Sierras',\n",
    "                       'S_Sierras_High',\n",
    "                       'S_Sierras_Low',\n",
    "                       'Greater_Yellowstone',\n",
    "                       'N_Co_Rockies',\n",
    "                       'SW_Mont',\n",
    "                       'SW_Co_Rockies',\n",
    "                       'GBasin',\n",
    "                       'N_Wasatch',\n",
    "                       'N_Cascade',\n",
    "                       'S_Wasatch',\n",
    "                       'SW_Mtns',\n",
    "                       'E_WA_N_Id_W_Mont',\n",
    "                       'S_Wyoming',\n",
    "                       'SE_Co_Rockies',\n",
    "                       'Sawtooth',\n",
    "                       'Ca_Coast',\n",
    "                       'E_Or',\n",
    "                       'N_Yellowstone',\n",
    "                       'S_Cascade',\n",
    "                       'Wa_Coast',\n",
    "                       'Greater_Glacier',\n",
    "                       'Or_Coast'\n",
    "                      ]\n",
    "SWE_new = {}\n",
    "for region in Region_list:\n",
    "    #The below file will serve as a starting point\n",
    "    SWE_new[region] = pd.read_hdf(cwd+'/Predictions/predictions'+prev_year+'-09-24.h5', key = region)\n",
    "    SWE_new[region].rename(columns = {prev_date:new_date}, inplace = True)\n",
    "    SWE_new[region].to_hdf(cwd+'/Predictions/predictions'+water_year+'-09-24.h5', key = region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#define start and end date for list of dates\n",
    "start_dt = date(2020, 10, 1)\n",
    "end_dt = date(2021, 7, 31)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can be altered to create list every n number of days by changing 7 to desired skip length\n",
    "def daterange(start_date, end_date):\n",
    "     for n in range(0, int((end_date - start_date).days) + 1, 7):\n",
    "        yield start_date + timedelta(n)\n",
    "         \n",
    "#create empty list to store dates\n",
    "datelist = []\n",
    "\n",
    "#append dates to list\n",
    "for dt in daterange(start_dt, end_dt):\n",
    "    #print(dt.strftime(\"%Y-%m-%d\"))\n",
    "    dt=dt.strftime('%Y-%m-%d')\n",
    "    datelist.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datelist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386868b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run the model through all time (data acqusition already completed)\n",
    "for date in datelist:\n",
    "    print('Updating SWE predictions for ', date)\n",
    "    #connect interactive script to Wasatch Snow module\n",
    "    Snow = NSM_SCA.NSM_SCA(cwd, date)\n",
    "    \n",
    "    #Go get SNOTEL observations -- currently saving to csv, change to H5,\n",
    "    #dd if self.data < 11-1-2022 and SWE = -9999, \n",
    "    Snow.Get_Monitoring_Data_Threaded()\n",
    "\n",
    "    #Get the prediction extent\n",
    "    bbox = Snow.getPredictionExtent()\n",
    "\n",
    "    #Initialize/Download the granules\n",
    "    Snow.initializeGranules(bbox, Snow.SCA_folder)\n",
    "\n",
    "    #Process observations into Model prediction ready format,\n",
    "    Snow.Data_Processing()\n",
    "\n",
    "    #Agument with SCA\n",
    "    Snow.augmentPredictionDFs()\n",
    "\n",
    "    #Make predictions\n",
    "    Snow.SWE_Predict()\n",
    "\n",
    "    #Make CONUS netCDF file, compressed.\n",
    "    Snow.netCDF_compressed(plot=False)\n",
    "\n",
    "    #Make GeoDataframe and plot, self.Geo_df() makes the geo df\n",
    "    # Snow.Geo_df()\n",
    "    # Snow.plot_interactive_SWE_comp(pinlat = 39.1, pinlong = -120, web = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run(cwd, date):\n",
    "    print('Updating SWE predictions for ', date)\n",
    "    #connect interactive script to Wasatch Snow module\n",
    "    Snow = NSM_SCA.NSM_SCA(cwd, date)\n",
    "\n",
    "    #Go get SNOTEL observations -- currently saving to csv, change to H5,\n",
    "    #dd if self.data < 11-1-2022 and SWE = -9999,\n",
    "    Snow.Get_Monitoring_Data_Threaded()\n",
    "\n",
    "    #Get the prediction extent\n",
    "    bbox = Snow.getPredictionExtent()\n",
    "\n",
    "    #Initialize/Download the granules\n",
    "    Snow.initializeGranules(bbox, Snow.SCA_folder)\n",
    "\n",
    "    #Process observations into Model prediction ready format,\n",
    "    Snow.Data_Processing()\n",
    "\n",
    "    #Agument with SCA\n",
    "    Snow.augmentPredictionDFs()\n",
    "\n",
    "    #Make predictions\n",
    "    Snow.SWE_Predict()\n",
    "\n",
    "    #Make CONUS netCDF file, compressed.\n",
    "    Snow.netCDF_compressed(plot=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for date in datelist[:4]:\n",
    "    %time run(cwd, date)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
